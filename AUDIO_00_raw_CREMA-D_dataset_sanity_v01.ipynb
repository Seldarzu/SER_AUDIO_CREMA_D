{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"29sGaNgPyXo0","executionInfo":{"status":"error","timestamp":1765487346558,"user_tz":-180,"elapsed":5167,"user":{"displayName":"Arzu AvcÄ±","userId":"03423494195407812851"}},"outputId":"788ee2c3-beb1-4c98-9150-a69934cd640e"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1844997610.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… Google Drive mounted successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 1: SETUP, IMPORTS, AND CONFIGURATION\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(\"âœ… Google Drive mounted successfully!\")\n","\n","# Library Imports\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import librosa\n","import librosa.display\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CONSTANTS AND DIRECTORIES\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","DATA_ROOT = '/content/drive/MyDrive/crema_d/AudioWAV/'\n","OUTPUT_DIR = './experiments_log/'\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# Seed for Reproducibility\n","SEED = 42\n","def set_seed(seed=SEED):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","set_seed(SEED)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"ğŸ–¥ï¸  Device: {DEVICE}\")\n","\n","# Emotion Label Mapping\n","LABEL_MAP = {'A': 0, 'D': 1, 'F': 2, 'H': 3, 'N': 4, 'S': 5}\n","NUM_CLASSES = len(LABEL_MAP)\n","EMOTION_LABELS = list(LABEL_MAP.keys())\n","\n","# Audio Feature Settings\n","TARGET_SR = 22050\n","MAX_LENGTH = 3.5\n","N_FFT = 1024\n","HOP_LENGTH = 512\n","N_MELS = 128\n","N_MFCC = 40\n","\n","# Global Variable for Normalization Stats\n","FEATURE_STATS = {}\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 2: DATA LOADING AND PREPROCESSING\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def load_audio(file_path):\n","    \"\"\"Load audio at target sample rate and pad/truncate to fixed length.\"\"\"\n","    audio, sr = librosa.load(file_path, sr=TARGET_SR)\n","    max_samples = int(MAX_LENGTH * sr)\n","    if len(audio) > max_samples:\n","        audio = audio[:max_samples]\n","    elif len(audio) < max_samples:\n","        padding = np.zeros(max_samples - len(audio))\n","        audio = np.concatenate((audio, padding))\n","    return audio\n","\n","# CREMA-D Emotion Code Mapper\n","EMOTION_CODE_MAPPER = {\n","    'ANG': 'A', 'DIS': 'D', 'FEA': 'F', 'HAP': 'H', 'NEU': 'N', 'SAD': 'S'\n","}\n","VALID_EMOTION_CODES_3CHAR = set(EMOTION_CODE_MAPPER.keys())\n","\n","def create_cremad_dataframe(data_root):\n","    \"\"\"Parse CREMA-D filenames and create DataFrame.\"\"\"\n","    file_paths = []\n","    actor_ids = []\n","    emotions = []\n","\n","    for filename in os.listdir(data_root):\n","        if filename.endswith(\".wav\"):\n","            base_filename = filename[:-4]\n","            parts = base_filename.split('_')\n","\n","            if len(parts) < 4:\n","                continue\n","\n","            emotion_code_3char = parts[2].upper()\n","\n","            if emotion_code_3char not in VALID_EMOTION_CODES_3CHAR:\n","                continue\n","\n","            emotion_code_1char = EMOTION_CODE_MAPPER[emotion_code_3char]\n","            actor_id = parts[0]\n","\n","            file_paths.append(os.path.join(data_root, filename))\n","            actor_ids.append(actor_id)\n","            emotions.append(emotion_code_1char)\n","\n","    df = pd.DataFrame({\n","        'path': file_paths,\n","        'actor_id': actor_ids,\n","        'emotion': emotions\n","    })\n","\n","    print(f\"ğŸ“Š Total files processed: {len(df)}\")\n","    return df\n","\n","# Load and Split Data (Speaker-Independent)\n","df_full = create_cremad_dataframe(DATA_ROOT)\n","actor_list = df_full['actor_id'].unique()\n","train_actors, test_actors = train_test_split(\n","    actor_list, test_size=0.2, random_state=SEED\n",")\n","\n","train_df = df_full[df_full['actor_id'].isin(train_actors)]\n","test_df = df_full[df_full['actor_id'].isin(test_actors)]\n","\n","# Calculate Class Weights\n","emotion_counts = train_df['emotion'].value_counts()\n","total_samples = emotion_counts.sum()\n","class_weights = total_samples / (NUM_CLASSES * emotion_counts)\n","sorted_weights = [class_weights[e] for e in EMOTION_LABELS]\n","WEIGHTS_TENSOR = torch.tensor(sorted_weights, dtype=torch.float32).to(DEVICE)\n","\n","print(f\"ğŸ‘¥ Training Speakers: {len(train_actors)}, Test Speakers: {len(test_actors)}\")\n","print(f\"âš–ï¸  Class Weights: {WEIGHTS_TENSOR.cpu().numpy()}\")\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 3: PREPROCESSING PIPELINES\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def preproc_A(audio):\n","    \"\"\"Pipeline A: Baseline - Simple Normalization.\"\"\"\n","    return audio\n","\n","def preproc_B(audio, sr=TARGET_SR):\n","    \"\"\"Pipeline B: Data Augmentation (Pitch Shift + Noise).\"\"\"\n","    pitch_shifted = librosa.effects.pitch_shift(\n","        audio, sr=sr, n_steps=random.uniform(-4, 4)\n","    )\n","    noise_amp = 0.005 * np.random.uniform() * np.amax(pitch_shifted)\n","    noisy_audio = pitch_shifted + noise_amp * np.random.normal(size=pitch_shifted.shape[0])\n","    return noisy_audio\n","\n","def preproc_C(audio):\n","    \"\"\"Pipeline C: RMS Normalization.\"\"\"\n","    rms = np.sqrt(np.mean(audio**2))\n","    normalized_audio = audio / (rms + 1e-6)\n","    return normalized_audio\n","\n","PREPROC_PIPELINES = {\n","    'A_Baseline': preproc_A,\n","    'B_Augmentation': preproc_B,\n","    'C_RMS_Norm': preproc_C\n","}\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 4: FEATURE EXTRACTION PIPELINES\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def features_A(audio, sr=TARGET_SR):\n","    \"\"\"Features A: MFCC + Delta + Delta-Delta.\"\"\"\n","    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n","    mfcc_delta = librosa.feature.delta(mfcc)\n","    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n","    combined_features = np.concatenate((mfcc, mfcc_delta, mfcc_delta2), axis=0)\n","    return combined_features.T\n","\n","def features_B(audio, sr=TARGET_SR):\n","    \"\"\"Features B: Log-Mel Spectrogram + Delta + Delta-Delta (3-Channel).\"\"\"\n","    mel_spectrogram = librosa.feature.melspectrogram(\n","        y=audio, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS\n","    )\n","    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","    delta = librosa.feature.delta(log_mel_spectrogram)\n","    delta2 = librosa.feature.delta(log_mel_spectrogram, order=2)\n","    combined_features = np.stack([log_mel_spectrogram, delta, delta2], axis=0)\n","    return combined_features\n","\n","def features_C(audio, sr=TARGET_SR):\n","    \"\"\"Features C: Hybrid MFCC + Spectral Features.\"\"\"\n","    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, n_fft=N_FFT, hop_length=HOP_LENGTH)\n","    chroma = librosa.feature.chroma_stft(y=audio, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)\n","    centroid = librosa.feature.spectral_centroid(y=audio, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)\n","    bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)\n","    combined_features = np.concatenate((mfcc, chroma, centroid, bandwidth), axis=0)\n","    return combined_features.T\n","\n","FEATURE_PIPELINES = {\n","    'A_MFCC_Delta': features_A,\n","    'B_LogMelSpec_3Ch': features_B,\n","    'C_Hybrid_1D': features_C\n","}\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 5: SPECAUGMENT CLASS (NEW!)\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","class SpecAugment:\n","    \"\"\"\n","    SpecAugment: Time and Frequency Masking for Spectrograms.\n","    Reference: Park et al., 2019 - SpecAugment\n","    \"\"\"\n","    def __init__(self, freq_mask_param=15, time_mask_param=20,\n","                 num_freq_masks=1, num_time_masks=2):\n","        self.freq_mask_param = freq_mask_param\n","        self.time_mask_param = time_mask_param\n","        self.num_freq_masks = num_freq_masks\n","        self.num_time_masks = num_time_masks\n","\n","    def __call__(self, spec):\n","        \"\"\"Apply SpecAugment to spectrogram tensor.\"\"\"\n","        spec = spec.clone()\n","\n","        if len(spec.shape) == 2:\n","            spec = spec.unsqueeze(0)\n","            squeeze_output = True\n","        else:\n","            squeeze_output = False\n","\n","        channels, n_freqs, n_frames = spec.shape\n","\n","        # Frequency Masking\n","        for _ in range(self.num_freq_masks):\n","            f = random.randint(0, self.freq_mask_param)\n","            f0 = random.randint(0, max(0, n_freqs - f))\n","            spec[:, f0:f0+f, :] = 0\n","\n","        # Time Masking\n","        for _ in range(self.num_time_masks):\n","            t = random.randint(0, min(self.time_mask_param, n_frames))\n","            t0 = random.randint(0, max(0, n_frames - t))\n","            spec[:, :, t0:t0+t] = 0\n","\n","        if squeeze_output:\n","            spec = spec.squeeze(0)\n","\n","        return spec\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 6: DATASET CLASS WITH SPECAUGMENT\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","class SERDataset(Dataset):\n","    \"\"\"Speech Emotion Recognition Dataset with Optional SpecAugment.\"\"\"\n","    def __init__(self, df, preproc_name, feature_name, stats=None, augment=False):\n","        self.df = df\n","        self.preproc = PREPROC_PIPELINES[preproc_name]\n","        self.feature_extractor = FEATURE_PIPELINES[feature_name]\n","        self.labels = [LABEL_MAP[l] for l in df['emotion']]\n","        self.stats = stats\n","        self.augment = augment\n","\n","        if self.augment:\n","            self.spec_augment = SpecAugment(\n","                freq_mask_param=15,\n","                time_mask_param=20,\n","                num_freq_masks=1,\n","                num_time_masks=2\n","            )\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        file_path = self.df.iloc[idx]['path']\n","        label = self.labels[idx]\n","\n","        audio = load_audio(file_path)\n","        audio = self.preproc(audio)\n","        features = self.feature_extractor(audio)\n","\n","        features_tensor = torch.tensor(features, dtype=torch.float32)\n","\n","        # Apply SpecAugment BEFORE normalization (only for 2D/3D features)\n","        if self.augment and len(features_tensor.shape) >= 2:\n","            features_tensor = self.spec_augment(features_tensor)\n","\n","        # Normalize\n","        if self.stats:\n","            mean = self.stats['mean']\n","            std = self.stats['std']\n","            features_tensor = (features_tensor - mean) / (std + 1e-6)\n","\n","        return features_tensor, torch.tensor(label, dtype=torch.long)\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 7: MODEL ARCHITECTURES\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","ACTIVATIONS = {\n","    'relu': nn.ReLU(),\n","    'leakyrelu': nn.LeakyReLU(),\n","    'gelu': nn.GELU()\n","}\n","\n","def calculate_conv_output_dim(input_tensor_shape, model):\n","    \"\"\"Calculate flattened dimension after conv layers.\"\"\"\n","    dummy_input = torch.zeros(1, *input_tensor_shape).to(DEVICE)\n","    with torch.no_grad():\n","        output = model.features(dummy_input)\n","        flattened_size = output.numel()\n","        return flattened_size // output.shape[0]\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# MODEL 1: CNN\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","class Model_CNN_A(nn.Module):\n","    def __init__(self, input_shape, num_classes, activation='relu', dropout_rate=0.5):\n","        super().__init__()\n","        act = ACTIVATIONS[activation]\n","\n","        self.features = nn.Sequential(\n","            nn.Conv2d(input_shape[0], 32, kernel_size=(3, 3), padding=1),\n","            nn.BatchNorm2d(32), act, nn.MaxPool2d(kernel_size=(2, 2)),\n","            nn.Dropout(dropout_rate),\n","            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n","            nn.BatchNorm2d(64), act, nn.MaxPool2d(kernel_size=(2, 2)),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        flattened_size = calculate_conv_output_dim(input_shape, self)\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(flattened_size, 256), act,\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# MODEL 2: LSTM\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","class Model_LSTM_A(nn.Module):\n","    def __init__(self, input_size, num_classes, hidden_size=128, num_layers=2,\n","                 activation='relu', dropout_rate=0.5):\n","        super().__init__()\n","        act = ACTIVATIONS[activation]\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,\n","                            bidirectional=True, dropout=dropout_rate if num_layers > 1 else 0)\n","        self.fc = nn.Sequential(\n","            nn.Linear(hidden_size * 2, 64), act,\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(64, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        lstm_out, _ = self.lstm(x)\n","        final_state = lstm_out[:, -1, :]\n","        out = self.fc(final_state)\n","        return out\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# MODEL 3: CRNN (Standard)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","class Model_CRNN_A(nn.Module):\n","    def __init__(self, input_shape, num_classes, hidden_size=64, num_layers=1,\n","                 activation='relu', dropout_rate=0.5):\n","        super().__init__()\n","        act = ACTIVATIONS[activation]\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(input_shape[0], 16, kernel_size=(3, 3), padding=1),\n","            nn.BatchNorm2d(16), act, nn.MaxPool2d(kernel_size=(1, 2)),\n","            nn.Dropout(dropout_rate)\n","        )\n","        self.features = self.conv\n","\n","        feature_size_after_conv = 16 * input_shape[1]\n","\n","        self.rnn = nn.LSTM(feature_size_after_conv, hidden_size, num_layers, batch_first=True,\n","                           bidirectional=True, dropout=dropout_rate if num_layers > 1 else 0)\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(hidden_size * 2, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = x.permute(0, 3, 1, 2)\n","        x = x.reshape(x.size(0), x.size(1), -1)\n","        rnn_out, _ = self.rnn(x)\n","        final_state = rnn_out[:, -1, :]\n","        out = self.classifier(final_state)\n","        return out\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# MODEL 4: CRNN WITH ATTENTION (NEW!)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","class Model_CRNN_Attention(nn.Module):\n","    \"\"\"CRNN with Self-Attention Mechanism.\"\"\"\n","    def __init__(self, input_shape, num_classes, hidden_size=64, num_layers=1,\n","                 activation='gelu', dropout_rate=0.5):\n","        super().__init__()\n","        act = ACTIVATIONS[activation]\n","\n","        # Convolutional Feature Extractor\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(input_shape[0], 32, kernel_size=(3, 3), padding=1),\n","            nn.BatchNorm2d(32), act, nn.MaxPool2d(kernel_size=(1, 2)),\n","            nn.Dropout(dropout_rate),\n","            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n","            nn.BatchNorm2d(64), act, nn.MaxPool2d(kernel_size=(1, 2)),\n","            nn.Dropout(dropout_rate)\n","        )\n","        self.features = self.conv\n","\n","        feature_size_after_conv = 64 * input_shape[1]\n","\n","        # Bidirectional LSTM\n","        self.rnn = nn.LSTM(\n","            feature_size_after_conv,\n","            hidden_size,\n","            num_layers,\n","            batch_first=True,\n","            bidirectional=True,\n","            dropout=dropout_rate if num_layers > 1 else 0\n","        )\n","\n","        # Self-Attention Mechanism\n","        self.attention = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.Tanh(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","\n","        # Classification Head\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hidden_size * 2, 128),\n","            act,\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(128, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        # Conv layers\n","        x = self.conv(x)\n","\n","        # Reshape for RNN\n","        x = x.permute(0, 3, 1, 2)\n","        x = x.reshape(x.size(0), x.size(1), -1)\n","\n","        # RNN\n","        rnn_out, _ = self.rnn(x)\n","\n","        # Attention mechanism\n","        attn_weights = self.attention(rnn_out)\n","        attn_weights = torch.softmax(attn_weights, dim=1)\n","\n","        # Weighted sum (context vector)\n","        context = torch.sum(rnn_out * attn_weights, dim=1)\n","\n","        # Classification\n","        out = self.classifier(context)\n","        return out\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 8: EVALUATION FUNCTION\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def evaluate_model(model, data_loader, stage, return_full=False):\n","    \"\"\"Evaluate model and return metrics.\"\"\"\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for features, labels in data_loader:\n","            if len(features.shape) > 2 and features.shape[1] == 1:\n","                features = features.squeeze(1)\n","            features, labels = features.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(features)\n","            _, predicted = torch.max(outputs.data, 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    if return_full:\n","        cm = confusion_matrix(all_labels, all_preds)\n","        report = classification_report(all_labels, all_preds, target_names=EMOTION_LABELS, zero_division=0)\n","        return accuracy, f1, cm, report\n","    return accuracy, f1\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 9: TRAINING FUNCTION WITH SCHEDULER (FIXED!)\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def train_and_evaluate(config, train_df, test_df, weights_tensor):\n","    \"\"\"Train and evaluate a single experiment configuration with LR Scheduler.\"\"\"\n","\n","    preproc_name = config['preproc']\n","    feature_name = config['features']\n","    batch_size = config['batch_size']\n","    epochs = config['epochs']\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"ğŸš€ EXPERIMENT: {config['name']}\")\n","    print(f\"{'='*80}\")\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # Step 1: Calculate Normalization Statistics\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    temp_dataset = SERDataset(train_df, preproc_name, feature_name, stats=None, augment=False)\n","    temp_loader = DataLoader(temp_dataset, batch_size=len(temp_dataset), shuffle=False)\n","    features_all, _ = next(iter(temp_loader))\n","\n","    if len(features_all.shape) > 2 and features_all.shape[1] == 1:\n","        features_all = features_all.squeeze(1)\n","\n","    mean = features_all.mean(dim=0, keepdim=True).to('cpu')\n","    std = features_all.std(dim=0, keepdim=True).to('cpu')\n","\n","    global FEATURE_STATS\n","    FEATURE_STATS[config['name']] = {'mean': mean, 'std': std}\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # Step 2: Create Data Loaders with SpecAugment\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    train_dataset = SERDataset(\n","        train_df, preproc_name, feature_name,\n","        stats=FEATURE_STATS[config['name']],\n","        augment=True  # Enable SpecAugment for training\n","    )\n","    test_dataset = SERDataset(\n","        test_df, preproc_name, feature_name,\n","        stats=FEATURE_STATS[config['name']],\n","        augment=False  # No augmentation for testing\n","    )\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # Step 3: Model Instantiation\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    sample_features, _ = next(iter(train_loader))\n","    input_shape = sample_features.shape\n","\n","    if len(input_shape) > 2 and input_shape[1] == 1:\n","        sample_features = sample_features.squeeze(1)\n","        input_shape = sample_features.shape\n","\n","    # Skip LSTM if input is 4D\n","    if config['model_type'] == 'LSTM_A' and len(input_shape) == 4:\n","        print(f\"âš ï¸  EXPERIMENT SKIPPED: LSTM cannot handle 4D input (Shape: {input_shape})\")\n","        return\n","\n","    model_input_shape = input_shape[1:]\n","\n","    if config['model_type'] == 'LSTM_A':\n","        model = Model_LSTM_A(\n","            input_size=model_input_shape[-1],\n","            num_classes=NUM_CLASSES,\n","            activation=config['activation'],\n","            dropout_rate=config['dropout']\n","        ).to(DEVICE)\n","    elif config['model_type'] == 'CNN_A':\n","        model = Model_CNN_A(\n","            input_shape=model_input_shape,\n","            num_classes=NUM_CLASSES,\n","            activation=config['activation'],\n","            dropout_rate=config['dropout']\n","        ).to(DEVICE)\n","    elif config['model_type'] == 'CRNN_A':\n","        model = Model_CRNN_A(\n","            input_shape=model_input_shape,\n","            num_classes=NUM_CLASSES,\n","            activation=config['activation'],\n","            dropout_rate=config['dropout']\n","        ).to(DEVICE)\n","    elif config['model_type'] == 'CRNN_Attention':\n","        model = Model_CRNN_Attention(\n","            input_shape=model_input_shape,\n","            num_classes=NUM_CLASSES,\n","            activation=config['activation'],\n","            dropout_rate=config['dropout']\n","        ).to(DEVICE)\n","    else:\n","        raise ValueError(f\"Unknown model type: {config['model_type']}\")\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # Step 4: Optimizer and Scheduler\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    if config['optimizer'] == 'Adam':\n","        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n","    elif config['optimizer'] == 'SGD':\n","        optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)\n","    elif config['optimizer'] == 'AdamW':\n","        optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n","\n","    criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n","\n","    # Learning Rate Scheduler (FIXED: removed 'verbose' parameter)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer,\n","        mode='min',\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6\n","    )\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # Step 5: Training Loop\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    best_val_accuracy = 0.0\n","    best_val_f1 = 0.0\n","    prev_lr = config['learning_rate']\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n","            if len(features.shape) > 2 and features.shape[1] == 1:\n","                features = features.squeeze(1)\n","\n","            features, labels = features.to(DEVICE), labels.to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            outputs = model(features)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        avg_train_loss = total_loss / len(train_loader)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for features, labels in test_loader:\n","                if len(features.shape) > 2 and features.shape[1] == 1:\n","                    features = features.squeeze(1)\n","\n","                features, labels = features.to(DEVICE), labels.to(DEVICE)\n","                outputs = model(features)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","        avg_val_loss = val_loss / len(test_loader)\n","        val_acc, val_f1 = evaluate_model(model, test_loader, 'Validation')\n","\n","        # Step the scheduler\n","        scheduler.step(avg_val_loss)\n","\n","        # Get current learning rate and check if it changed\n","        current_lr = optimizer.param_groups[0]['lr']\n","        if current_lr != prev_lr:\n","            print(f\"ğŸ“‰ Learning rate reduced: {prev_lr:.6f} â†’ {current_lr:.6f}\")\n","            prev_lr = current_lr\n","\n","        print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.4f} | \"\n","              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n","              f\"Val F1: {val_f1:.4f} | LR: {current_lr:.6f}\")\n","\n","        # Save best model\n","        if val_acc > best_val_accuracy:\n","            best_val_accuracy = val_acc\n","            best_val_f1 = val_f1\n","            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f\"{config['name']}_best.pth\"))\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # Step 6: Test Evaluation\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, f\"{config['name']}_best.pth\")))\n","    test_acc, test_f1, cm, report = evaluate_model(model, test_loader, 'Test', return_full=True)\n","\n","    print(f\"\\n{'â”€'*80}\")\n","    print(f\"ğŸ“Š FINAL TEST RESULTS:\")\n","    print(f\"   Accuracy: {test_acc:.4f}\")\n","    print(f\"   F1-Score: {test_f1:.4f}\")\n","    print(f\"{'â”€'*80}\\n\")\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # Step 7: Save Results\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    new_row = {\n","        'experiment_name': config['name'],\n","        'preprocessing_pipeline': preproc_name,\n","        'feature_pipeline': feature_name,\n","        'model_type': config['model_type'],\n","        'activation': config['activation'],\n","        'optimizer': config['optimizer'],\n","        'learning_rate': config['learning_rate'],\n","        'dropout': config['dropout'],\n","        'batch_size': batch_size,\n","        'epochs': epochs,\n","        'val_accuracy': best_val_accuracy,\n","        'val_f1': best_val_f1,\n","        'test_accuracy': test_acc,\n","        'test_f1': test_f1,\n","        'confusion_matrix': cm.tolist()\n","    }\n","\n","    global RESULTS_DF\n","    RESULTS_DF = pd.concat([RESULTS_DF, pd.DataFrame([new_row])], ignore_index=True)\n","    RESULTS_DF.to_csv('experiments_results_checkpoint.csv', index=False)\n","    print(\"ğŸ’¾ Results saved to checkpoint file.\")\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 10: LOAD EXISTING RESULTS OR CREATE NEW DATAFRAME\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","csv_path = 'experiments_results_checkpoint.csv'\n","\n","if os.path.exists(csv_path):\n","    RESULTS_DF = pd.read_csv(csv_path)\n","    print(f\"âœ… Existing results loaded: {len(RESULTS_DF)} experiments found.\")\n","else:\n","    RESULTS_DF = pd.DataFrame(columns=[\n","        'experiment_name', 'preprocessing_pipeline', 'feature_pipeline', 'model_type',\n","        'activation', 'optimizer', 'learning_rate', 'dropout', 'batch_size', 'epochs',\n","        'val_accuracy', 'val_f1', 'test_accuracy', 'test_f1', 'confusion_matrix'\n","    ])\n","    print(\"ğŸ†• New results dataframe created.\")\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 11: RUN NEW EXPERIMENTS\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# EXPERIMENT 9: CRNN with Attention + SpecAugment\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","config_e09 = {\n","    'name': 'EXP_E09_CRNN_Attention_SpecAug',\n","    'preproc': 'A_Baseline',\n","    'features': 'B_LogMelSpec_3Ch',\n","    'model_type': 'CRNN_Attention',\n","    'activation': 'gelu',\n","    'optimizer': 'AdamW',\n","    'learning_rate': 1e-3,\n","    'dropout': 0.5,\n","    'batch_size': 32,\n","    'epochs': 30\n","}\n","\n","print(f\"\\n{'#'*80}\")\n","print(f\"# STARTING: {config_e09['name']}\")\n","print(f\"{'#'*80}\\n\")\n","\n","train_and_evaluate(config_e09, train_df, test_df, WEIGHTS_TENSOR)\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# EXPERIMENT 10: Ultimate Configuration (RMS + Attention + SpecAugment)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","config_e10 = {\n","    'name': 'EXP_E10_CRNN_Attention_RMS_SpecAug',\n","    'preproc': 'C_RMS_Norm',\n","    'features': 'B_LogMelSpec_3Ch',\n","    'model_type': 'CRNN_Attention',\n","    'activation': 'gelu',\n","    'optimizer': 'AdamW',\n","    'learning_rate': 2e-3,\n","    'dropout': 0.4,\n","    'batch_size': 24,\n","    'epochs': 40\n","}\n","\n","print(f\"\\n{'#'*80}\")\n","print(f\"# STARTING: {config_e10['name']}\")\n","print(f\"{'#'*80}\\n\")\n","\n","train_and_evaluate(config_e10, train_df, test_df, WEIGHTS_TENSOR)\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# CELL 12: FINAL VISUALIZATION AND ANALYSIS\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","if RESULTS_DF.empty:\n","    print(\"âš ï¸  No results to visualize!\")\n","else:\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"ğŸ“Š ALL EXPERIMENT RESULTS SUMMARY\")\n","    print(\"=\"*80 + \"\\n\")\n","\n","    # Display summary table\n","    summary_cols = ['experiment_name', 'test_accuracy', 'test_f1', 'model_type']\n","    summary_table = RESULTS_DF[summary_cols].sort_values(by='test_f1', ascending=False)\n","    print(summary_table.to_string(index=False))\n","\n","    # Find best model\n","    RESULTS_DF['test_f1'] = pd.to_numeric(RESULTS_DF['test_f1'], errors='coerce')\n","    best_idx = RESULTS_DF['test_f1'].idxmax()\n","    best_row = RESULTS_DF.loc[best_idx]\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"ğŸ† BEST MODEL: {best_row['experiment_name']}\")\n","    print(f\"   Test F1-Score: {best_row['test_f1']:.4f}\")\n","    print(f\"   Test Accuracy: {best_row['test_accuracy']:.4f}\")\n","    print(f\"   Model Type: {best_row['model_type']}\")\n","    print(f\"{'='*80}\\n\")\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # PLOT 1: F1-Score Comparison Bar Chart\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    plt.figure(figsize=(14, 8))\n","    sorted_df = RESULTS_DF.sort_values('test_f1', ascending=True)\n","\n","    colors = ['#FF6B6B' if 'Attention' not in name else '#4ECDC4'\n","              for name in sorted_df['experiment_name']]\n","\n","    bars = plt.barh(sorted_df['experiment_name'], sorted_df['test_f1'], color=colors)\n","\n","    # Add value labels on bars\n","    for i, (bar, val) in enumerate(zip(bars, sorted_df['test_f1'])):\n","        plt.text(val + 0.005, i, f'{val:.3f}', va='center', fontsize=9, fontweight='bold')\n","\n","    plt.xlabel('Test F1-Score', fontsize=12, fontweight='bold')\n","    plt.ylabel('Experiment Name', fontsize=12, fontweight='bold')\n","    plt.title('Speech Emotion Recognition: All Experiments F1-Score Comparison',\n","              fontsize=14, fontweight='bold', pad=20)\n","    plt.grid(axis='x', linestyle='--', alpha=0.3)\n","    plt.tight_layout()\n","    plt.savefig('experiment_comparison.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # PLOT 2: Best Model Confusion Matrix\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    if 'confusion_matrix' in best_row and best_row['confusion_matrix'] is not None:\n","        try:\n","            # Handle string representation of list\n","            if isinstance(best_row['confusion_matrix'], str):\n","                import ast\n","                cm = np.array(ast.literal_eval(best_row['confusion_matrix']))\n","            else:\n","                cm = np.array(best_row['confusion_matrix'])\n","\n","            plt.figure(figsize=(10, 8))\n","            sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd',\n","                       xticklabels=EMOTION_LABELS, yticklabels=EMOTION_LABELS,\n","                       cbar_kws={'label': 'Count'}, linewidths=0.5, linecolor='gray')\n","            plt.title(f'Confusion Matrix: {best_row[\"experiment_name\"]}\\n'\n","                     f'Test F1: {best_row[\"test_f1\"]:.4f}',\n","                     fontsize=14, fontweight='bold', pad=20)\n","            plt.ylabel('True Emotion', fontsize=12, fontweight='bold')\n","            plt.xlabel('Predicted Emotion', fontsize=12, fontweight='bold')\n","            plt.tight_layout()\n","            plt.savefig('best_model_confusion_matrix.png', dpi=300, bbox_inches='tight')\n","            plt.show()\n","        except Exception as e:\n","            print(f\"âš ï¸  Could not plot confusion matrix: {e}\")\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # PLOT 3: Model Type Performance Comparison\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","    plt.figure(figsize=(12, 6))\n","    model_type_performance = RESULTS_DF.groupby('model_type')['test_f1'].mean().sort_values()\n","\n","    colors_model = plt.cm.viridis(np.linspace(0, 1, len(model_type_performance)))\n","    bars = plt.barh(model_type_performance.index, model_type_performance.values, color=colors_model)\n","\n","    for i, (bar, val) in enumerate(zip(bars, model_type_performance.values)):\n","        plt.text(val + 0.005, i, f'{val:.3f}', va='center', fontsize=10, fontweight='bold')\n","\n","    plt.xlabel('Average Test F1-Score', fontsize=12, fontweight='bold')\n","    plt.ylabel('Model Architecture', fontsize=12, fontweight='bold')\n","    plt.title('Performance by Model Architecture Type', fontsize=14, fontweight='bold', pad=20)\n","    plt.grid(axis='x', linestyle='--', alpha=0.3)\n","    plt.tight_layout()\n","    plt.savefig('model_type_comparison.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","\n","    print(\"\\nâœ… All visualizations saved successfully!\")\n","    print(\"   - experiment_comparison.png\")\n","    print(\"   - best_model_confusion_matrix.png\")\n","    print(\"   - model_type_comparison.png\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"ğŸ‰ COMPLETE! All experiments finished and results saved.\")\n","print(\"=\"*80)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNE+/0kSsxDXg5oCiaJOjxK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}